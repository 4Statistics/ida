<style>@import url(style.css);</style>
[Introduction to Data Analysis](index.html "Course index")

# 9.2. Smoothing

To test a few smoothing algorithms, we are going to mimick the [Quandl API][q-api] by scraping and downloading short time ####.

http://www.quandl.com/help/r

```{r packages, message=FALSE, warning=FALSE}
# Load packages.
packages <- c("changepoint", "downloader", "ggplot2", "lubridate", "plyr", "reshape")
packages <- lapply(packages, FUN = function(x) {
  if(!require(x, character.only = TRUE)) {
    install.packages(x)
    library(x, character.only = TRUE)
  }
})
```

[q-api]: http://www.quandl.com/api

```{r quandl-scrape}
# 
link = "http://www.quandl.com/api/v1/datasets/STATCHINA/E0403.csv?&trim_start=1952-12-31&trim_end=2011-12-31&sort_order=desc"
file = "data/china.sectors.5211.txt"
# Download
if(!file.exists(file)) download(link, file, mode = "wb")
# Read data, skipping row numbers.
data <- read.csv(file, stringsAsFactors = FALSE)
# Fix names.
names(data) <- c("Year", "Active", "Employed", "Primary", "Secondary", "Tertiary",
                 "P1", "P2", "P3")
# Reshape.
data <- melt(data, id = "Year", variable = "Industry")
# Turn years to proper dates.
data$Year <- ymd(data$Year)
```

```{r plot}
# Working population, millions
qplot(data = subset(data, Industry %in% c("Primary", "Secondary", "Tertiary")),
      y = value / 10^2, x = year(Year), colour = Industry, fill = Industry,
      position = "stack", geom = "area") +
  labs(y = "Million Employees")
```

```{r plot2}
#
qplot(data = subset(data, Population %in% c("Primary", "Secondary", "Tertiary")),
      y = value / 10^3, x = Year, colour = Population, fill = Population,
      position = "stack", geom = "area") + 
    scale_y_continuous(labels = comma) + labs(y = "Million")
```

## Changepoints

The next example is based on online ratings of TV shows. The example was first coded at [Diffuse Prior][dp-geoscpt], and Andy at [Premier Soccer Stats][pss-geoscpt] have even coded it as an [interactive graph][pss-shiny] with [Shiny][shiny]. We'll start by scraping the data [from GEOS][tww-geos]. The series under scrutiny is Aaron Sorkin's _The West Wing_.

[dp-geoscpt]: http://diffuseprior.wordpress.com/2013/04/30/kalkalash-pinpointing-the-moments-the-simpsons-became-less-cromulent/
[pss-geoscpt]: http://www.premiersoccerstats.com/wordpress/?p=1380
[pss-shiny]: http://glimmer.rstudio.com/pssguy/TVShowRatings/
[shiny]: http://www.rstudio.com/shiny/
[tww-geos]: http://www.geos.tv/index.php/list?sid=179&collection=all

```{r tww-data}
file = "data/geos.tww.txt"
if(!file.exists(file)) {
  # Parse HTML content.
  html <- htmlParse("http://www.geos.tv/index.php/list?sid=179&collection=all")
  # Select table on id.
  html <- xpathApply(html, "//table[@id='collectionTable']")[[1]]
  # Convert to dataset.
  data <- readHTMLTable(html)
  # First data rows.
  head(data)
  # Save local copy.
  write.csv(data[, -3], file, row.names = FALSE)
}
# Read local copy.
data <- read.csv(file, stringsAsFactors = FALSE)
# Check result.
str(data)
```

`Mean` is the average rating of each episode, so we have a parameter, and `Count` is the number of votes on each episode, so we have a sample size. Using the equation for the standard error, $SE = \frac{SD}{\sqrt{N}}$, we will calculate the "margin of error" of each rating. Note that the distribution of the ratings is not normal due to a few episodes having received very high ratings.

```{r tww-se}
# Convert means from text.
data$mu <- as.numeric(substr(data$Mean, 0, 4))
# Compute standard errors.
data$se <- with(data, sd(mu) / sqrt(as.numeric(Count)))
```

The last step that we take with the data is to add the season number to be able to discriminate them visually later on. Each season of _The West Wing_ has 22 episodes, except for two special cases (the final season has 23 episodes and one show is a film special). We use the remainder of a division by 22 to compute seasons, fix the special cases, and factor the variable for plotting purposes.

```{r tww-seasons}
# Compute season.
data$season <- 1 + (data$X - 1) %/% 22
# Fix special cases.
data$season[which(data$season > 7)] <- c(7, NA)
# Factor variable.
data$season <- factor(data$season)
```

The final plot uses 95% and 99% confidence intervals to visualize (some of the) uncertainty.

```{r tww-plot}
g <- qplot(data = data, x = X, y = mu, colour = season, geom = "point") + 
  geom_linerange(aes(ymin = mu - 1.96*se, ymax = mu + 1.96*se), alpha = .5) +
  geom_linerange(aes(ymin = mu - 2.58*se, ymax = mu + 2.58*se), alpha = .5) +
  scale_colour_brewer("Season", palette = "Set1") +
  scale_x_continuous(breaks = seq(1, 156, 22)) +
  theme(legend.position = "top") +
  labs(y = "Mean rating", x = "Episode")
g
```

The plot would be more useful with average ratings per season, which are easy to retrieve with the `ddply()` function. The minimum and maximum episode numbers are also computed to be able to plot horizontal segments for each season. Since we saved the previous plot as a `ggplot2` object, adding the lines only requires to code the extra graphical element and add it on top of the saved elements.

```{r tww-means}
# Compute season means.
means <- ddply(data, .(season), summarise, 
      mean = mean(mu), 
      xmin = min(X), 
      xmax = max(X))
# Add means to plot.
g + geom_segment(data = means, 
                 aes(x = xmin, xend = xmax, y = mean, yend = mean))
```

```{r tww-cpt}
cpt <- cpt.mean(data$mu, method = 'PELT')
seg <- data.frame(cpt = attr(m, "param.est"))
seg$xmax <- attr(cpt, "cpts")
seg$xmin <- c(0, seg$xmax[-length(seg$xmax)])

g + geom_segment(data = seg, aes(x = xmin, xend = xmax, y = mean, yend = mean), color = "black")
```

By the way, if you find a way to predict this kind of data, [let Netflix know][nfp] and [submit your work early][nfp-nyt].

[nfp]: http://www.netflixprize.com/
[nfp-nyt]: bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/

## Smoothing: LOESS and cubic splines

```{r libs}
# Load MASS package (provides "rlm" function).
library(MASS)
# Load splines package (provides "ns" function).
library(splines)
```

```{r smoothers}
g <- qplot(data = data, x = X, y = mu, alpha = I(0.5), geom = "line") + 
  geom_hline(y = mean(data$mu), linetype = "dashed") +
  scale_x_continuous(breaks = seq(1, 156, 22)) +
  labs(y = "Mean rating", x = "Episode")

g + geom_smooth(fill = "steelblue", se = FALSE) + aes(colour = season, y = c(0, diff(mu)))
g + geom_smooth(method="rlm", formula = y ~ ns(x, 8))
```

The mathematics of [smoothed splines](http://www.johnmarquess.com/?p=111) involve several methods of computation. When there is a lot of data, we fit local polynomials with `loess`. When there is less data, as with time series over a few decades, a generalized additive model (`gam`) provides a more robust smoother.

Let's smooth the series using `ggplot2` to [make it easy](http://docs.ggplot2.org/current/stat_smooth.html). We build a x-y canvas that will plot each weapon type in a different color, and we get the axes and titles ready before plotting. We now add a smoothed curve to the plot canvas, using a [LOESS function][wiki-loess].

A different smoother uses a [generalized additive model][wiki-gam] to plot the basic cubic spline of the series (see `?gam` and `?ns` for details). The general idea is to predict each data point of the series $x$ at $t_k$ from the vector of regressors $(t_0, t_1, t_2, ..., t_{k-2}, t_{k-1})$, i.e. from the past values of the series. The `MASS` library provides the `rlm` method, and the `splines` library provides the `ns()` function.

[wiki-loess]: https://en.wikipedia.org/wiki/Local_regression
[wiki-gam]: https://en.wikipedia.org/wiki/Generalized_additive_model

Note how the change in smoothing algorithm affects the aspect of the curve for handguns. On the same topic, see [Kieran Healy's plots and code](https://github.com/kjhealy/assault-deaths) on state-by-state trends and the United States vers other OECD countries.

> __Next__: [Practice](093_practice.html).
