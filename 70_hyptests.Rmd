<style>@import url(style.css);</style>

# 7. Confidence intervals

The summary statistics that you calculate from your sample are generalizable to the whole population if you select a level of confidence and use the sample size to determine the margin of error of your measurements.

There is [more than one reason](https://solomonmessing.wordpress.com/2012/03/04/visualization-series-insight-from-cleveland-and-tufte-on-plotting-numeric-data-by-groups/) not to use bar plots or pie charts. One of them is that you might want to plot an estimate instead of a value, by showing the size of the standard error on the graph.

Here's an example of [confidence intervals](http://wiki.stdout.org/rcookbook/Graphs/Plotting%20means%20and%20error%20bars%20(ggplot2)/) that uses, as most of our work, `ggplot2` graphics.

More reliable standard errors can be obtained through bootstrap resampling.

__Bootstrap resampling__ is a way to run the same test over simulated data obtained through sampling among permutations of observations, to see whether the effect persists if we use only some of the data. It provides a further test of what we might interpret from the *t*-test, or any other statistical procedure available for bootstrapping.

Resampling happens through simulations (or "replications") of alternative samples.

__Simulation__ generally aims at adding various possible scenarios to a statistical test or model: randomness can be used to let the software compute a procedure over slightly different parameters, again and again, until we reach a satisfying number of possibles and consider their results altogether. 

We can [bootstrap the standard error](https://solomonmessing.wordpress.com/2011/11/26/putting-it-all-together-concise-code-to-make-dotplots-with-weighted-bootstrapped-standard-errors/) by measuring it over simulated samples.

## *t*-test

Here's a rough overview of the *t*-test.

The __*t*-test__ estimates whether the means of two groups differ, based on the size of the observed difference and on the size of the sample used. It uses the *t*-distribution to approach the normal distribution, and assumes that the groups are independent and identically distributed (i.e. of identical variance) to compute the *p*-value of the test.

The point of the test is to verify whether observable differences have statistical power.

At low sample size, there is a risk to accept as significant some differences that could be caused by __sampling error__. To test that possibility, we consider the probability of the __null hypothesis__ $H_0$, which states that the difference $\mu_1 - \mu_2$ is equal to 0. Rejecting $H_0$ while it is actually true is known as the Type I Error.

The converse error can be equally dangerous: accepting the absence of a difference while there is one, a Type II Error, is one of the risks of working with low-$N$ samples, because their low __statistical power__ threatens the validity of the test. Furthermore, no statistical test can account for pre-processing errors in the data itself.

Let's see, for instance, if support for democracy has increased since 1990.

<!-- Deploy the data from the QOG.
  http://wiki.stdout.org/rcookbook/Statistical%20analysis/t-test/ -->

## Tables

__Tables__ with Chi-squared tests and odds ratios.

__Bar plots__: beurkâ€¦ except for [mosaic bars](https://learnr.wordpress.com/2009/04/02/ggplot2-marimekko-replacement-overlapping-bars/), perhaps.

__Mosaic plots__: yay! See examples [here](https://learnr.wordpress.com/2009/03/29/ggplot2_marimekko_mosaic_chart/) and [there](http://is-r.tumblr.com/post/33290921643/simple-marimekko-mosaic-plots)). Definitely more appropriate to speak of modal groups, frequencies and fractions of a population.