<style>@import url(style.css);</style>
<small>[Introduction to Data Analysis](index.html)</small>

# 5.3. *k*-means

To finish, here's one more method to classify your data into clusters: hierarchical clustering.

```{r prepare-data}
# Load original data subset.
load("data/qog_b.Rda")
# Reorder the dataset by fertility rate.
qog_hc <- qog_b[order(qog_b$births), ]
# Store country names in row names.
rownames(qog_hc) <- qog_hc$cname
# Remove country codes and names from the matrix.
qog_hc <- qog_hc[, -c(1:2)]
```

Hierarchical clustering methods are based on distance matrixes, i.e. distances between each row of the data matrix. The distance increases with differences in the data. These distances can be used to plot the observations, and to separate them into rectangular groups at the bottom of a graph called the dendrogram, where each observation belongs to one or more clusters.

```{r distance-matrix}
# Create the distance matrix.
d <- dist(qog_hc, method="euclidean")
# Clusters
hfit <- hclust(d, method="ward")
# Plot dendrogram.
plot(hfit)
```

The information provided by the dendrogram becomes more clear once we build some limits between groups, as is made possible by the within-group architecture of the plot.

```{r clusters}
# Cutting dendrogram into five clusters.
clus5 <- cutree(hfit, 5)
# Plot dendrogram with red borders around the five clusters.
rect.hclust(hfit, k=5, border="red")
```

There's [tons of ways to plot dendrograms](https://gastonsanchez.wordpress.com/2012/10/03/7-ways-to-plot-dendrograms-in-r/). Let's have fun, to finish, with one of them.

```{r fan-plot}
# Create a vector of colors for the five clusters.
mypal <- c("#556270", "#4ECDC4", "#1B676B", "#FF6B6B", "#C44D58")
# Fan plot.
plot(as.phylo(hfit), type = "fan", tip.color = mypal[clus5], label.offset = 1)
# Rerun so that the size of the labels represents the (log of the) fertility rate.
plot(as.phylo(hfit), type = "fan", tip.color = mypal[clus5], label.offset = 1,
     cex = log(qog_hc$births, 2.5))
```

One element of hierarchical clustering draws on $k$-means, which is a handy tool when you want to add ellipses around clusters on your PCA plots. It calculates distances between observations so as to identify groups that largely correspond to what your PCA biplot has already shown.

```{r kmeans}
# k-means clustering with five clusters.
hfit2 <- kmeans(qog_hc, 5)
# Cluster plot.
clusplot(qog_hc, hfit2$cluster, color = TRUE, shade = TRUE, labels = 2, lines = 0)
# Describe the hfit matrix.
hfit2
```

<!-- Brief overview [simple examples](http://www.r-chart.com/2010/09/bot-botany-k-means-and-ggplot2.html) and more complex ones like  Focus on [clustergrams and diagnostics](http://www.r-statistics.com/2010/06/clustergram-visualization-and-diagnostics-for-cluster-analysis-r-code/) for cluster analysis. -->

Now, as an exercise, recreate a similar analysis as the one shown in this session, using the measure of happiness available from the QOG Basic dataset. Steal all the code you need from the session.

> __Next week__: [Distributions](60_distributions.html).
