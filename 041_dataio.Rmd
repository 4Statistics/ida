<style>@import url(style.css);</style>
[Introduction to Data Analysis](index.html "Course index")

# 4.1. Imports and exports

This section introduces data import and export (or "I/O", for "input/output") with R.

```{r packages, message=FALSE, warning=FALSE}
# Load packages.
packages <- c("countrycode", "downloader", "foreign", "ggplot2", "RCurl")
packages <- lapply(packages, FUN = function(x) {
  if(!require(x, character.only = TRUE)) {
    install.packages(x)
    library(x, character.only = TRUE)
  }
})
```

## Downloading a CSV file

We are going to use the [Daily Kos Elections' presidential results by congressional district for the 2012 and 2008 elections](http://www.dailykos.com/story/2012/11/19/1163009/-Daily-Kos-Elections-presidential-results-by-congressional-district-for-the-2012-2008-elections?detail=hide), for which the data is accessible as a [Google spreadsheet](https://docs.google.com/spreadsheet/pub?key=0Av8O-dN2giY6dEFCOFZ4ZnlKS0x3M3Y0WHd5aWFDWkE&single=true&gid=0&output=html). The [Google Docs API documentation](http://code.google.com/apis/spreadsheets/) tells us that we can get the data in CSV format through a URL request that includes the identifier key of the spreadsheet and the format specification `output=csv`.

```{r dk-url, message=FALSE}
# Create a filename for the dataset.
file = "data/dailykos-votes.txt"
# Store the address of the spreadsheet.
link = "https://docs.google.com/spreadsheet/pub?key=0Av8O-dN2giY6dEFCOFZ4ZnlKS0x3M3Y0WHd5aWFDWkE&output=csv"
```

We now need to get the data from that address, using the `getURL` command from the `RCurl` package to fetch the online spreadsheet from the `dk.link` object in which we stored the link. The `ssl.verifypeer` option is required to [avoid an issue](http://christophergandrud.blogspot.fr/2012/06/update-to-data-on-github-post-solution.html) with the SSL certification used by `HTTPS` links.

Note that the `getURL()` command is in a conditional statement that avoids downloading the same file again and again if you already have it.

When the file is fetched from online, we convert the result, which is a large text file, to a proper CSV (comma-separated values) file. We specify that we do not want strings converted to factors, i.e. that we do not want a numeric structure for the text variables.

```{r dk-grab}
# Download dataset.
if (!file.exists(file)) {
  message("Dowloading the data...")
  # Download and read HTML spreadsheet.
  html <- textConnection(getURL(link, ssl.verifypeer = FALSE))
  # Convert and export CSV spreadsheet.
  write.csv(read.csv(html), file)
}
# Open file.
dkos <- read.csv(file, stringsAsFactors = FALSE)
# Check result.
str(dkos)
```

We finally inspect the result by looking at the structure of the dataset with `str` and the first few rows of data with `head`. We finally build a quick plot showing the distribution of Obama vote shares in congressional districts held by Democrat and Republican congressmen.

```{r dk-inspection}
# List first data rows.
head(dkos)
# Plot distribution of Obama vote share.
qplot(data = dkos, x = Obama.2012, 
      fill = Party, colour = Party, alpha = I(.75), geom = "density")
```

## The Quality of Government dataset

Here's the code to download the Quality of Government (QOG) dataset that we will use in future classes. Install the `downloader` package before running the code (by now, you should know how to install a package). The first code block will download the QOG codebook if it is not found in the data folder.

```{r qog-codebook, results='hide'}
# Download Quality of Government Basic codebook.
file = "data/qog-codebook.pdf"
if(!file.exists(file)) {
  url = "http://www.qogdata.pol.gu.se/codebook/codebook_standard_20110406.pdf"
  download(url, file, mode = "wb")
}
```

The next code block checks whether you have the comma-separated values version of the data. If not, it downloads the QOG Standard cross-sectional dataset in Stata format, opens it using the `read.dta()` function from the `foreign` library, and converts it from that source.

```{r qog-data}
# Download Quality of Government Basic dataset.
file = "data/qog-cs.txt"
if(!file.exists(file)) {
  if(!file.exists(dta <- "data/qog-cs.dta")) {
    url = "http://www.qogdata.pol.gu.se/data/qog_std_cs.dta"
    download(url, dta, mode = "wb")
  }
  write.csv(read.dta(dta), file)
}
# Open local copy.
qog <- read.csv(file, stringsAsFactors = FALSE, header = TRUE)
```

QOG datasets feature a wide range of variables from various sources, including UN data, World Development Indicators and several research datasets. The Stata format has the advantage of holding variable and value labels, but we will assume that you have opened the codebook and are working with the full variable definitions.

The example below uses data from the Barro and Lee and Maddison studies. Another advantage of Quality of Government data is that it comes with several country identifiers that allow to match external data to it, as we do below by adding continental origin based on ISO-3C country codes.

```{r qog-plot}
# Add geographic continents using UN country codes.
continent <- factor(countrycode(qog$ccodealp, "iso3c", "continent"))
# Plot log-GDP/capita and female education, weighted by population (2002).
qplot(data = qog, y = log(wdi_gdpc), x = bl_asyf25, 
      colour = continent, size = mad_pop / 10^3, geom = "point") +
  scale_size_area(max_size = 24) + scale_colour_brewer(palette = "Set1") +
  labs(y = "log GDP/capita", x = "Female schooling years")
```

## Scraping

For our purposes, scraping is the idea of taking information from online sources and to process it into a dataset. [Zarino Zappia](https://scraperwiki.com/profiles/zarino/), the creator of the ScraperWiki website, has written great examples of such scrapers in the Python programming language, as with his [scraper of Occupy protest locations](https://scraperwiki.com/scrapers/occupy_protest_locations/) (using Wikipedia sources; see also the [chronological map](https://views.scraperwiki.com/run/occupy_protest_locations_1/) based on the data).

Let's replicate one of his examples and scrape the [life expectancy index](http://hdrstats.undp.org/en/indicators/72206.html) calculated by the United Nations. The [Python scraper](https://scraperwiki.com/scrapers/un_life_expectancy_scores/) can be emulated in R at low programming cost.

Examples: scraping [RealClearPolitics](http://is-r.tumblr.com/post/36059986744/gathering-realclearpolitics-polling-trends-with-xml) and [Wikipedia](http://is-r.tumblr.com/post/36945206190/using-xml-to-grab-tables-from-the-web) with `XML`. Read on how scraping and [data APIs](http://stats.stackexchange.com/questions/12670/data-apis-feeds-available-as-packages-in-r) makes [data sociology](http://quanti.hypotheses.org/647/) possible. Here's a nice example: [scraping Craigslist](http://quanti.hypotheses.org/724/) (in French).

> __Next__: [Reshapes and aggregates](042_reshaping.html).
