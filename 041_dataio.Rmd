<style>@import url(style.css);</style>
<small>[Introduction to Data Analysis](index.html)</small>

# 4.1. Imports and exports

This section introduces data import and export (or "I/O", for "input/output") with R.

## Downloading a CSV file

We are going to use the [Daily Kos Elections' presidential results by congressional district for the 2012 and 2008 elections](http://www.dailykos.com/story/2012/11/19/1163009/-Daily-Kos-Elections-presidential-results-by-congressional-district-for-the-2012-2008-elections?detail=hide), for which the data is accessible as a [Google spreadsheet](https://docs.google.com/spreadsheet/pub?key=0Av8O-dN2giY6dEFCOFZ4ZnlKS0x3M3Y0WHd5aWFDWkE&single=true&gid=0&output=html). The [Google Docs API documentation](http://code.google.com/apis/spreadsheets/) tells us that we can get the data in CSV format through a URL request that includes the identifier key of the spreadsheet and the format specification `output=csv`.

```{r dk-url, message=FALSE}
# Load RCurl package.
library(RCurl)
# Store the address of the spreadsheet.
dk.link <- "https://docs.google.com/spreadsheet/pub?key=0Av8O-dN2giY6dEFCOFZ4ZnlKS0x3M3Y0WHd5aWFDWkE&output=csv"
```

We now need to get the data from that address, using the `getURL` command from the `RCurl` package to fetch the online spreadsheet from the `dk.link` object in which we stored the link. The `ssl.verifypeer` option is required to [avoid an issue](http://christophergandrud.blogspot.fr/2012/06/update-to-data-on-github-post-solution.html) with the SSL certification used by `HTTPS` links.

Note that the `getURL` command is in a conditional statement that avoids downloading the same file again and again if you already have it.

When the file is fetched from online, we convert the result, which is a large text file, to a proper CSV (comma-separated values) file. We specify that we do not want strings converted to factors, i.e. that we do not want a numeric structure for the text variables.

```{r dk-grab}
if (!file.exists("data/dailykos.csv")) {
  # Download spreadsheet.
  dk.html <- getURL(dk.link, ssl.verifypeer = FALSE)
  # Convert result to proper dataset, leaving strings as such.
  dk.data <- read.csv(textConnection(dk.html), stringsAsFactors = FALSE)
  # Save local copy.
  write.csv(dk.data, file = "data/dailykos.csv")
} else {
  # Open local copy.
  dk.data <- read.csv("data/dailykos.csv")
}
```

We finally inspect the result by looking at the structure of the dataset with `str` and the first few rows of data with `head`.

```{r dk-inspection}
# Have a look at the final result.
str(dk.data)
# List the first data rows.
head(dk.data)
```

## The Quality of Government dataset

Here's the code to download a dataset that we will use in future classes.

```{r qog-basic-dl, results='hide'}
if(!file.exists("data/qog_basic_cs.csv")) {
  if(!file.exists("data")) {
   dir.create("data")
  }
  if(!require(RCurl)) {
    install.packages("RCurl")
    library(RCurl)
  }
  url <- "http://www.qog.pol.gu.se/digitalAssets/1373/1373417_qog_basic_cs_csv_120608.csv"
  qog <- getURL(url)
  qog <- read.csv(textConnection(qog))
  write.csv(qog, file = "data/qog_basic_cs.csv")
} else {
  # Open local copy.
  qog <- read.csv("data/qog_basic_cs.csv")
}
```

## Other formats

The `Hmisc` and `foreign` packages do a wonderful job at converting datasets in proprietary formats.

Here's another example using Excel. [The data](http://www.inflationtrends.com/data/Raw_data.xlsx) are about inflation trends in the United States, as [made available](http://www.reddit.com/r/datasets/comments/13ww5b/have_some_data_on_us_food_and_fuel_prices_median/) by user [data_junkie](http://www.reddit.com/user/data_junkie) at Reddit on the [/r/datasets](http://www.reddit.com/r/datasets/) channel. Check out Reddit if you don't know about it already.

Because the data are formatted as an Excel 2007 file (`.xlsx`), you will need to load the [xlsx](http://cran.r-project.org/web/packages/xlsx/) package for the next commands to work. You would usually export the file as a CSV if it's only a single file that you need to import; the method shown below is useful only for larger batch imports.

The import is done through the `read.xlsx` command, with the `header` option set to `FALSE` because the first row of the spreadsheet is not a header (it holds the first row of data, not the variable names). After checking on the import, we subset the `infl` data frame to columns 3 and 4 and rename them.

```{r xlsx1, message=FALSE}
# Load the package.
library(xlsx)
# Import the data.
if (file.exists("data/us_inflation.xlsx")) {
  # Open local copy.
  infl <- read.xlsx("data/us_inflation.xlsx","median_income", header = FALSE)
} else {
  print("Cannot find dataset, loading from web...")
  # Retrieve file.
  infl <- read.xlsx("http://www.inflationtrends.com/data/Raw_data.xlsx", header = FALSE)
  # Save local copy.
  write.xlsx(infl, file = "data/us_inflation.xlsx")
}
# Check the result.
str(infl)
# Subset to columns 3 to 4.
infl <- infl[3:4]
# Add column names.
names(infl) <- c("year","medinc")
```

Let's plot the data to take a look. All we have here is median income by year: a single time series. We use `ggplot2` to draw a quick plot as a [simple line](http://docs.ggplot2.org/current/geom_line.html). The command understands the first argument as the x-variable and the second one as the y-variable. The `data` and `geom` arguments do the rest of the job.

```{r xlsx2, message=FALSE}
# A quick plot.
qplot(year, medinc, data = infl, geom = "line")
```

The end of the series shows a punctuation (you guessed it: welcome to the subprime crisis). Let's focus on the most recent years, then.

```{r xlsx3}
# Check how far the series goes.
summary(infl$year)
# Plot the last twenty years.
qplot(year,medinc,data=subset(infl, year > 1992),geom="line")
```

Finally, since the series seems to increase exponentially, let's have a look at the logged series, where the y-axis uses a logarithmic scale.

```{r xlsx4}
# A quick plot with a log scale.
qplot(year,medinc,data=infl,geom="line",log="y")
```

> __Next__: [Reshapes and aggregates](042_reshaping.html).
