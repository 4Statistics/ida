<style>@import url(style.css);</style>
[Introduction to Data Analysis](index.html "Course index")

# 9.1. Autocorrelation

[Jay Ulfelder][ju], a political scientist who has been credited for publishing [rather accurate lists][fp] of most likely military coups in recent years, wrote on that topic: "Unsurprisingly, coups also turn out to be a recurrent problem; the risk is higher in countries that have experienced other coup attempts in the past several years, a factor common to the top eight countries on this list."

[ju]: https://dartthrowingchimp.wordpress.com/
[fp]: http://blog.foreignpolicy.com/posts/2013/03/25/stats_junkie_successfully_predicts_african_coup_again

This observation is applicable in many different contexts with relatively slow-moving quantities: the unemployment rate, for instance, is highly redundant from one month to another, modulo a small percent change; GDP approximates itself from one year to another; and so on. In brief, the best way to predict an event at time $t$ is often to look at that same event at $t-1, t - 2, ..., t - k$, with $k$ lags.

This form of temporal dependence is called autocorrelation, or serial correlation, in the context of time series. They can be shown in specific plot arrangements, like correlograms, or expressed through notions like marginal change (which mathematically relies on derivatives), lagged values, or detrended series. We will sample a few of these methods below.

## ...

The same principle can be used to get data from [sources like Twitter][sw-ms]. An interesting time series can be created [from a Twitter account logging Beijing pollution data][bc-tap-1]: there was a [pollution peak][bc-tap-2] in January 2013, and the city [looked like][bj] *Blade Runner* with [more fog][ss]; staying in Beijing then amounted to [smoking 1.5 to 3 cigarettes a day][ss-smok].

[bc-tap-1]: http://brainchronicle.blogspot.co.uk/2012/07/twitter-analysis-of-air-pollution-in.html
[bc-tap-2]: http://brainchronicle.blogspot.fr/2013/01/air-quality-analysis-from-beijing.html
[bj]: http://kateoplis.tumblr.com/post/40555052298/nope-this-is-not-a-still-from-blade-runner-its
[ss]: http://simplystatistics.org/2013/01/14/welcome-to-the-smog-ocalypse/
[ss-smok]: http://simplystatistics.org/2011/12/14/smoking-is-a-choice-breathing-is-not/

Unfortunately, the Twitter API is now less open than it used to be, so we will not be able to access the data publicly. I have saved the results of a [recent scrape][beijing-scraper] for your viewing pleasure, as the U.S. Embassy has not turned the logging machine down even when pressured so. Read first on the [Air Quality Index][aqi] (AQI) used to measure pollution information.

[beijing-scraper]: 4_twitter.R
[aqi]: http://airnow.gov/index.cfm?action=aqibasics.aqi

```{r twitter-pollution}
# Target data source.
link = "https://raw.github.com/briatte/ida/master/data/beijing.aqi.2013.txt"
file = "data/beijing.aqi.2013.txt"
if(!file.exists(file)) download("", file)
# Read CSV file.
bp <- read.csv(file, stringsAsFactors = FALSE)
# Check result.
head(bp)
bp$time <- strptime(bp$time, format = "%Y-%m-%d %T")
ggplot(data = bp, aes(x = time, y = PM)) +
  geom_line(color = "gray80") +
  geom_point(color = "blue", alpha = .5) +
  geom_smooth(fill = "lightblue") +
  labs(x = NULL, y = "Fine particles (PM2.5) 24hr avg")
```

There's plenty of other ways to look at the data. Make sure to plot a smoothed trend. I stopped coding at the one below.

```{r unodc-spline, warning=FALSE}
# Load MASS package (provides "rlm" function).
library(MASS)
# Load splines package (provides "ns" function).
library(splines)
# Plot canvas.
fig <- ggplot(unodc, aes(y = rate, x = year, group = country, color = country, fill = country))
# Spline, 2-length knots.
fig <- fig + geom_smooth(method="rlm", formula = y ~ ns(x, 2), alpha = .25)
# Check result.
fig + ylab("Homicide rate per 100,000 population") + xlab("Year")
```

## Lagging a time series

Let's examine a simple example where $t$ (time) varies from 2001 to 2005, and $x_t = {1, 2, ..., 5}$. This series can be turned into a time series object with many different packages. One of them is called `zoo`: it creates a single object made of core data, and a time index, both of which are accessible through dedicated functions.

```{r ts}
# Create year data.
df <- data.frame(t = 2001:2005, x = (1:5)^2)
# Check result.
str(df)
# Create zoo object.
df <- with(df, zoo(x, t))
# Check result.
str(df)
# Extract core data.
coredata(df)
# Extract time index.
index(df)
```

Teetor, ch. 14, offers a quick introduction to the topic of time series from that perspective, using the `zoo` and `xts` packages. The `lag()` function, for example, is convenient to effortlessly obtain the lagged values $x_{-k}, ..., x_{t-1}, x_{t+1}, x_{t+k}$ from $x_{t}$ in a time series object. In the examples below, we show the `x` sequence lagged at $k = (-2, -1, +1, +2)$:

```{r ts-lag}
# Original time series.
df
# Lagged at k = -2: shifts series right by two years.
lag(df, -2, na = TRUE)
# Lagged at k = -1: shifts series right by one year.
lag(df, -1, na = TRUE)
# Lagged at k = +1: shifts series left by one year.
lag(df, 1, na = TRUE)
# Lagged at k = +2: shifts series left by two years.
lag(df, 2, na = TRUE)
```

When your interest is in _differencing_ a time series, you want to subtract $x_{t-1}$ to all its values $x_{t}$, that is, obtain the net difference between two values separated by one time period. This is equivalent to asking for the marginal change in $x$ at every point $t$ of the curve $x(t)$. The `diff()` function offers a very convenient way to obtain lagged differences:

```{r ts-diff}
# Recall the time series and lagged values.
t(cbind(df, lag(df, -1)))
# Compute the successive differences.
diff(df)
# Show differences with both time series.
t(cbind(df, lag(df, -1), diff(df)))
```

Note that the rate of change is mathematically equivalent to taking the ratio of the derivative by the function: the relative rate of change is $\frac{f'(x)}{f(x)}$, and the percentage rate of change is $100 \cdot \frac{f'(x)}{f(x)}$. What we now want to compute is the actual change, using the differences of each series divided by $x_{t-1}$.

## Detrending

The previous plots show how a time series at $x_{t}$ can be lagged by taking $x_{t-1}$ into account. The final step is to take many lags into account to control for autocorrelation, i.e. the full series of correlations $\rho_k$ with $k = 1, 2, ..., k$ lags, in order to detrend the data. This is done by regressing the data onto its time index, for which an example follows.

...

Teetor, ch. 14.13-14.21, further explains how to model a time series from its past values, using plots and elements similar to those shown here. There is also a great book and R package by [Shumway and Stoffer](http://www.stat.pitt.edu/stoffer/tsa3/), which goes into greater length about linear time series (and the nonlinear book is forthcoming this year).

> __Next__: [Smoothing](092_smoothing.html).
